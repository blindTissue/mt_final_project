{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T21:26:19.981342Z",
     "start_time": "2024-12-05T21:26:19.195341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int):\n",
    "        \"\"\"\n",
    "        Encoder for seq2seq model.\n",
    "        Args:\n",
    "            input_dim: Dimensionality of input vectors\n",
    "            hidden_dim: Hidden state size of the LSTM\n",
    "            num_layers: Number of LSTM layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_dim)\n",
    "        Returns:\n",
    "            encoder_outputs: Outputs for each time step (batch_size, seq_len, hidden_dim)\n",
    "            hidden: Tuple of (h_n, c_n) (last hidden and cell states)\n",
    "        \"\"\"\n",
    "        encoder_outputs, hidden = self.lstm(x)\n",
    "        return encoder_outputs, hidden\n",
    "\n",
    "\n",
    "class Seq2SeqDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, output_dim: int, num_layers: int):\n",
    "        \"\"\"\n",
    "        Decoder for seq2seq model.\n",
    "        Args:\n",
    "            hidden_dim: Dimensionality of the encoded hidden state\n",
    "            output_dim: Dimensionality of output vectors (same as input_dim)\n",
    "            num_layers: Number of LSTM layers (should match encoder)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor to the decoder (batch_size, seq_len, hidden_dim)\n",
    "            hidden: Tuple of (h_n, c_n) from the encoder\n",
    "        Returns:\n",
    "            outputs: Decoded sequence (batch_size, seq_len, output_dim)\n",
    "        \"\"\"\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        outputs = self.fc(lstm_out)\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int):\n",
    "        \"\"\"\n",
    "        Combines encoder and decoder into a seq2seq model.\n",
    "        Args:\n",
    "            input_dim: Dimensionality of input vectors\n",
    "            hidden_dim: Hidden state size\n",
    "            num_layers: Number of LSTM layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = Seq2SeqEncoder(input_dim, hidden_dim, num_layers)\n",
    "        self.decoder = Seq2SeqDecoder(hidden_dim, input_dim, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input sequence of shape (batch_size, seq_len, input_dim)\n",
    "        Returns:\n",
    "            output: Reconstructed sequence of shape (batch_size, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        encoder_outputs, hidden = self.encoder(x)\n",
    "\n",
    "        # Decoder (Use encoder outputs as initial input)\n",
    "        output, _ = self.decoder(encoder_outputs, hidden)\n",
    "        return output"
   ],
   "id": "6a84eae9eca33a44",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T21:29:47.933946Z",
     "start_time": "2024-12-05T21:29:47.024740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "def load_data(max_length=512):\n",
    "    print(\"Loading dataset from HuggingFace...\")\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "    train_texts = dataset['train']['text']\n",
    "\n",
    "    filtered_texts = [text for text in train_texts if text.strip() and len(text.split()) <= max_length]\n",
    "    print(f\"Processed {len(filtered_texts)} samples after filtering\")\n",
    "\n",
    "    return filtered_texts"
   ],
   "id": "8173ee5fb845dd9d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T21:30:03.460335Z",
     "start_time": "2024-12-05T21:30:01.734158Z"
    }
   },
   "cell_type": "code",
   "source": "texts = load_data()",
   "id": "fbc077403c625bee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from HuggingFace...\n",
      "Processed 23758 samples after filtering\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T21:30:30.520751Z",
     "start_time": "2024-12-05T21:30:30.518059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(texts))\n",
    "print(texts[1000])"
   ],
   "id": "d084ab15bdf15a5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23758\n",
      " After Deconstruction and Ghost , Townsend announced a new album , Casualties of Cool , with which he started to work after the release of Epicloud . The album features Ché Aimee Dorval ( from Ki ) on vocals and Morgan Ågren on drums . Townsend described the album sounds like \" haunted Johnny Cash songs \" and \" late night music \" , highlighting it will be different than anything he has done before . Townsend referred the music of the album to be \" closest to his heart \" at this point of his life , and that it is an important and satisfying project he doesn 't want to rush . \n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T21:37:45.403122Z",
     "start_time": "2024-12-05T21:37:21.115477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class LlamaIntermediateLayerExtractor:\n",
    "    def __init__(self, model_1b_path, model_3b_path):\n",
    "        \"\"\"\n",
    "        Initialize extractor for Llama 3.2 1b and 3b models\n",
    "\n",
    "        Args:\n",
    "            model_1b_path (str): Path or HuggingFace model ID for 1b model\n",
    "            model_3b_path (str): Path or HuggingFace model ID for 3b model\n",
    "        \"\"\"\n",
    "        # Load models and tokenizers\n",
    "        self.model_1b = AutoModelForCausalLM.from_pretrained(model_1b_path)\n",
    "        self.model_3b = AutoModelForCausalLM.from_pretrained(model_3b_path)\n",
    "\n",
    "        self.tokenizer_1b = AutoTokenizer.from_pretrained(\"tokenizers/meta-llama/llama-3.2-1B\")\n",
    "        self.tokenizer_3b = AutoTokenizer.from_pretrained(\"tokenizers/meta-llama/llama-3.2-3B\")\n",
    "\n",
    "        # Set models to evaluation mode\n",
    "        self.model_1b.eval()\n",
    "        self.model_3b.eval()\n",
    "\n",
    "        # Hooks to capture intermediate layer outputs\n",
    "        self.intermediate_output_1b = None\n",
    "        self.intermediate_output_3b = None\n",
    "\n",
    "    def _register_1b_hook(self):\n",
    "        \"\"\"Register hook for 1b model's 10th layer\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            self.intermediate_output_1b = output[0]  # Typically, first element is hidden states\n",
    "\n",
    "        # Assuming transformer layers are in model.model.layers or similar\n",
    "        # You might need to adjust this path based on your specific model structure\n",
    "        target_layer = self.model_1b.model.layers[9]  # 0-indexed, so 10th layer is index 9\n",
    "        self.hook_1b = target_layer.register_forward_hook(hook)\n",
    "\n",
    "    def _register_3b_hook(self):\n",
    "        \"\"\"Register hook for 3b model's 18th layer\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            self.intermediate_output_3b = output[0]\n",
    "\n",
    "        # Adjust this path based on your specific model structure\n",
    "        target_layer = self.model_3b.model.layers[17]  # 0-indexed, so 18th layer is index 17\n",
    "        self.hook_3b = target_layer.register_forward_hook(hook)\n",
    "\n",
    "    def extract_intermediate_representations(self, text_chunks, max_length=512):\n",
    "        \"\"\"\n",
    "        Extract intermediate representations for given text chunks\n",
    "\n",
    "        Args:\n",
    "            text_chunks (list): List of text chunks to process\n",
    "            max_length (int): Maximum token length to process\n",
    "\n",
    "        Returns:\n",
    "            tuple: (intermediate representations for 1b, intermediate representations for 3b)\n",
    "        \"\"\"\n",
    "        # Reset intermediate outputs\n",
    "        self.intermediate_output_1b = None\n",
    "        self.intermediate_output_3b = None\n",
    "\n",
    "        # Register hooks\n",
    "        self._register_1b_hook()\n",
    "        self._register_3b_hook()\n",
    "\n",
    "        # Prepare to collect representations\n",
    "        repr_1b_list = []\n",
    "        repr_3b_list = []\n",
    "\n",
    "        try:\n",
    "            for chunk in text_chunks:\n",
    "                # Tokenize and process 1b model\n",
    "                inputs_1b = self.tokenizer_1b(\n",
    "                    chunk,\n",
    "                    return_tensors='pt',\n",
    "                    truncation=True,\n",
    "                    max_length=max_length\n",
    "                )\n",
    "\n",
    "                # Tokenize and process 3b model\n",
    "                inputs_3b = self.tokenizer_3b(\n",
    "                    chunk,\n",
    "                    return_tensors='pt',\n",
    "                    truncation=True,\n",
    "                    max_length=max_length\n",
    "                )\n",
    "\n",
    "                # Forward pass to trigger hooks\n",
    "                with torch.no_grad():\n",
    "                    _ = self.model_1b(**inputs_1b)\n",
    "                    _ = self.model_3b(**inputs_3b)\n",
    "\n",
    "                # Store intermediate representations\n",
    "                if self.intermediate_output_1b is not None:\n",
    "                    repr_1b_list.append(self.intermediate_output_1b.detach())\n",
    "\n",
    "                if self.intermediate_output_3b is not None:\n",
    "                    repr_3b_list.append(self.intermediate_output_3b.detach())\n",
    "\n",
    "        finally:\n",
    "            # Remove hooks to prevent memory leaks\n",
    "            self.hook_1b.remove()\n",
    "            self.hook_3b.remove()\n",
    "\n",
    "        return repr_1b_list, repr_3b_list\n",
    "\n",
    "    def align_representations(self, repr_1b, repr_3b):\n",
    "        \"\"\"\n",
    "        Align intermediate representations\n",
    "\n",
    "        Args:\n",
    "            repr_1b (list): Intermediate representations from 1b model\n",
    "            repr_3b (list): Intermediate representations from 3b model\n",
    "\n",
    "        Returns:\n",
    "            tuple: Aligned and processed representations\n",
    "        \"\"\"\n",
    "        # Basic alignment strategy:\n",
    "        # 1. Ensure consistent dimensionality\n",
    "        # 2. Potential dimensionality reduction\n",
    "        # 3. Normalize representations\n",
    "\n",
    "        # Check representation compatibility\n",
    "        assert len(repr_1b) == len(repr_3b), \"Representations must be paired\"\n",
    "\n",
    "        aligned_repr_1b = []\n",
    "        aligned_repr_3b = []\n",
    "\n",
    "        for r1, r3 in zip(repr_1b, repr_3b):\n",
    "            # Example simple alignment (you may need more sophisticated method)\n",
    "            # Potential strategies:\n",
    "            # - Linear projection\n",
    "            # - PCA\n",
    "            # - Embedding matching\n",
    "\n",
    "            # Basic normalization\n",
    "            r1_norm = (r1 - r1.mean()) / r1.std()\n",
    "            r3_norm = (r3 - r3.mean()) / r3.std()\n",
    "\n",
    "            aligned_repr_1b.append(r1_norm)\n",
    "            aligned_repr_3b.append(r3_norm)\n",
    "\n",
    "        return aligned_repr_1b, aligned_repr_3b\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Replace with your actual model paths\n",
    "    MODEL_1B_PATH = \"models/meta-llama/llama-3.2-1B\"\n",
    "    MODEL_3B_PATH = \"models/meta-llama/llama-3.2-3B\"\n",
    "\n",
    "    # Text chunks to process\n",
    "    text_chunks = [\n",
    "        \"This is the first text chunk.\",\n",
    "        \"Another interesting piece of text goes here.\",\n",
    "        \"And a third chunk for good measure.\"\n",
    "    ]\n",
    "\n",
    "    # Create extractor\n",
    "    extractor = LlamaIntermediateLayerExtractor(MODEL_1B_PATH, MODEL_3B_PATH)\n",
    "\n",
    "    # Extract representations\n",
    "    repr_1b, repr_3b = extractor.extract_intermediate_representations(text_chunks)\n",
    "\n",
    "    # Align representations\n",
    "    aligned_1b, aligned_3b = extractor.align_representations(repr_1b, repr_3b)\n",
    "\n",
    "    # Print some basic information\n",
    "    print(f\"Number of 1B representations: {len(aligned_1b)}\")\n",
    "    print(f\"Number of 3B representations: {len(aligned_3b)}\")\n",
    "    print(f\"1B representation shape: {aligned_1b[0].shape}\")\n",
    "    print(f\"3B representation shape: {aligned_3b[0].shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "df8a4ee61c920712",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ebc4c042f5144dabd82b105f01a64f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1B representations: 3\n",
      "Number of 3B representations: 3\n",
      "1B representation shape: torch.Size([1, 8, 2048])\n",
      "3B representation shape: torch.Size([1, 8, 3072])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T06:05:28.949317Z",
     "start_time": "2024-12-06T06:05:16.734664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q = \"What is the meaning of life?\"\n",
    "extractor = LlamaIntermediateLayerExtractor(\"meta-llama/llama-3.2-1B\", \"meta-llama/llama-3.2-3B\")\n",
    "repr_1b, repr_3b = extractor.extract_intermediate_representations([q])"
   ],
   "id": "46bfdc691a189e73",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ee11dbcccb744bfb021af8cca2ce3aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T06:05:59.969517Z",
     "start_time": "2024-12-06T06:05:57.461601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llama_1b_s = AutoModelForCausalLM.from_pretrained(\"meta-llama/llama-3.2-1B\")\n",
    "llama_1b_s.model.layers"
   ],
   "id": "652efa497f29843c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-15): 16 x LlamaDecoderLayer(\n",
       "    (self_attn): LlamaSdpaAttention(\n",
       "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "      (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "      (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "      (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T06:06:21.046633Z",
     "start_time": "2024-12-06T06:06:21.044285Z"
    }
   },
   "cell_type": "code",
   "source": "llama_1b_s.model.layers = llama_1b_s.model.layers[10:]",
   "id": "dc9441e059b61e8c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T06:06:48.583547Z",
     "start_time": "2024-12-06T06:06:47.248511Z"
    }
   },
   "cell_type": "code",
   "source": "llama_1b = AutoModelForCausalLM.from_pretrained(\"meta-llama/llama-3.2-1B\")",
   "id": "c914f5916d6ac3f5",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T06:07:30.321736Z",
     "start_time": "2024-12-06T06:07:29.688115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/llama-3.2-1B\")\n",
    "tokenized = tokenizer.tokenize(q)\n",
    "out = llama_1b(**tokenizer(q, return_tensors=\"pt\"))"
   ],
   "id": "6262da4c50655657",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T06:07:47.357098Z",
     "start_time": "2024-12-06T06:07:47.355058Z"
    }
   },
   "cell_type": "code",
   "source": "out.logits.shape",
   "id": "a54e84e188245fd6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 128256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T06:14:54.752171Z",
     "start_time": "2024-12-06T06:14:54.747760Z"
    }
   },
   "cell_type": "code",
   "source": "out.logits",
   "id": "ffc685e62e750299",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7597],\n",
       "         [11.6842,  8.4082,  7.3702,  ..., -0.2775, -0.2776, -0.2776],\n",
       "         [ 8.2786,  9.0717,  6.5711,  ..., -0.1754, -0.1750, -0.1752],\n",
       "         ...,\n",
       "         [ 7.0296, 10.0401,  5.8869,  ..., -0.8870, -0.8870, -0.8872],\n",
       "         [15.6388, 12.5399, 11.7565,  ...,  0.9495,  0.9491,  0.9493],\n",
       "         [10.9356,  7.8670,  8.9850,  ..., -0.4730, -0.4730, -0.4729]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T06:14:27.640613Z",
     "start_time": "2024-12-06T06:14:27.189886Z"
    }
   },
   "cell_type": "code",
   "source": "inter_out = llama_1b_s(inputs_embeds = repr_1b[0])",
   "id": "b832319bceb22f17",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T06:14:37.805763Z",
     "start_time": "2024-12-06T06:14:37.802271Z"
    }
   },
   "cell_type": "code",
   "source": "inter_out.logits",
   "id": "b310fc0c7d1b1737",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7597],\n",
       "         [11.6842,  8.4082,  7.3702,  ..., -0.2775, -0.2776, -0.2776],\n",
       "         [ 8.2786,  9.0717,  6.5711,  ..., -0.1754, -0.1750, -0.1752],\n",
       "         ...,\n",
       "         [ 7.0296, 10.0401,  5.8869,  ..., -0.8870, -0.8870, -0.8872],\n",
       "         [15.6388, 12.5399, 11.7565,  ...,  0.9495,  0.9491,  0.9493],\n",
       "         [10.9356,  7.8670,  8.9850,  ..., -0.4730, -0.4730, -0.4729]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T06:29:34.411051Z",
     "start_time": "2024-12-06T06:29:34.208080Z"
    }
   },
   "cell_type": "code",
   "source": "seq2seqmodel = Seq2SeqModel(3072, 2048, 1)",
   "id": "d6a3bd36128781d5",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "for sentence in texts:\n",
   "id": "8398dee62ba9ce1c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
