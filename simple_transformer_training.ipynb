{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-06T21:04:47.059960Z",
     "start_time": "2024-12-06T21:04:46.412942Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.onnx.symbolic_opset9 import cosine_similarity"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T21:04:47.063197Z",
     "start_time": "2024-12-06T21:04:47.062023Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "796c8d8952294653",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T21:04:47.131178Z",
     "start_time": "2024-12-06T21:04:47.129301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEmbedder(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, layers):\n",
    "        super(TransformerEmbedder, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=8, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=layers)\n",
    "        self.activation = nn.GELU()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = nn.Transformer().generate_square_subsequent_mask(x.shape[1])\n",
    "        #print(mask.shape)\n",
    "        x = self.transformer_encoder(x, mask=mask, is_causal=True)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ],
   "id": "ca6c525c725f29a8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T21:04:47.144817Z",
     "start_time": "2024-12-06T21:04:47.143486Z"
    }
   },
   "cell_type": "code",
   "source": "device = 'mps'",
   "id": "c7345cdc32a8efad",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T21:04:47.440049Z",
     "start_time": "2024-12-06T21:04:47.150542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class LlamaIntermediateLayerExtractor:\n",
    "    def __init__(self, model_1b_path, model_3b_path):\n",
    "        \"\"\"\n",
    "        Initialize extractor for Llama 3.2 1b and 3b models\n",
    "\n",
    "        Args:\n",
    "            model_1b_path (str): Path or HuggingFace model ID for 1b model\n",
    "            model_3b_path (str): Path or HuggingFace model ID for 3b model\n",
    "        \"\"\"\n",
    "        # Load models and tokenizers\n",
    "        self.model_1b = AutoModelForCausalLM.from_pretrained(model_1b_path).to(device)\n",
    "        self.model_3b = AutoModelForCausalLM.from_pretrained(model_3b_path).to(device)\n",
    "\n",
    "        self.tokenizer_1b = AutoTokenizer.from_pretrained(\"tokenizers/meta-llama/llama-3.2-1B\")\n",
    "        self.tokenizer_3b = AutoTokenizer.from_pretrained(\"tokenizers/meta-llama/llama-3.2-3B\")\n",
    "\n",
    "        # Set models to evaluation mode\n",
    "        self.model_1b.eval()\n",
    "        self.model_3b.eval()\n",
    "\n",
    "        # Hooks to capture intermediate layer outputs\n",
    "        self.intermediate_output_1b = None\n",
    "        self.intermediate_output_3b = None\n",
    "\n",
    "    def _register_1b_hook(self):\n",
    "        \"\"\"Register hook for 1b model's 10th layer\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            self.intermediate_output_1b = output[0]  # Typically, first element is hidden states\n",
    "\n",
    "        # Assuming transformer layers are in model.model.layers or similar\n",
    "        # You might need to adjust this path based on your specific model structure\n",
    "        target_layer = self.model_1b.model.layers[9]  # 0-indexed, so 10th layer is index 9\n",
    "        self.hook_1b = target_layer.register_forward_hook(hook)\n",
    "\n",
    "    def _register_3b_hook(self):\n",
    "        \"\"\"Register hook for 3b model's 18th layer\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            self.intermediate_output_3b = output[0]\n",
    "\n",
    "        # Adjust this path based on your specific model structure\n",
    "        target_layer = self.model_3b.model.layers[17]  # 0-indexed, so 18th layer is index 17\n",
    "        self.hook_3b = target_layer.register_forward_hook(hook)\n",
    "\n",
    "    def extract_intermediate_representations(self, text_chunks, max_length=512):\n",
    "        \"\"\"\n",
    "        Extract intermediate representations for given text chunks\n",
    "\n",
    "        Args:\n",
    "            text_chunks (list): List of text chunks to process\n",
    "            max_length (int): Maximum token length to process\n",
    "\n",
    "        Returns:\n",
    "            tuple: (intermediate representations for 1b, intermediate representations for 3b)\n",
    "        \"\"\"\n",
    "        # Reset intermediate outputs\n",
    "        self.intermediate_output_1b = None\n",
    "        self.intermediate_output_3b = None\n",
    "\n",
    "        # Register hooks\n",
    "        self._register_1b_hook()\n",
    "        self._register_3b_hook()\n",
    "\n",
    "        # Prepare to collect representations\n",
    "        repr_1b_list = []\n",
    "        repr_3b_list = []\n",
    "\n",
    "        try:\n",
    "            for chunk in text_chunks:\n",
    "                # Tokenize and process 1b model\n",
    "                inputs_1b = self.tokenizer_1b(\n",
    "                    chunk,\n",
    "                    return_tensors='pt',\n",
    "                    truncation=True,\n",
    "                    max_length=max_length\n",
    "                ).to(device)\n",
    "\n",
    "                # Tokenize and process 3b model\n",
    "                inputs_3b = self.tokenizer_3b(\n",
    "                    chunk,\n",
    "                    return_tensors='pt',\n",
    "                    truncation=True,\n",
    "                    max_length=max_length\n",
    "                ).to(device)\n",
    "\n",
    "                # Forward pass to trigger hooks\n",
    "                with torch.no_grad():\n",
    "                    _ = self.model_1b(**inputs_1b)\n",
    "                    _ = self.model_3b(**inputs_3b)\n",
    "\n",
    "                # Store intermediate representations\n",
    "                if self.intermediate_output_1b is not None:\n",
    "                    repr_1b_list.append(self.intermediate_output_1b.detach())\n",
    "\n",
    "                if self.intermediate_output_3b is not None:\n",
    "                    repr_3b_list.append(self.intermediate_output_3b.detach())\n",
    "\n",
    "        finally:\n",
    "            # Remove hooks to prevent memory leaks\n",
    "            self.hook_1b.remove()\n",
    "            self.hook_3b.remove()\n",
    "\n",
    "        return repr_1b_list, repr_3b_list\n",
    "\n"
   ],
   "id": "fdccd97811d5c752",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T21:04:58.177531Z",
     "start_time": "2024-12-06T21:04:47.443610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_1B_PATH = \"models/meta-llama/llama-3.2-1B\"\n",
    "MODEL_3B_PATH = \"models/meta-llama/llama-3.2-3B\"\n",
    "extractor = LlamaIntermediateLayerExtractor(MODEL_1B_PATH, MODEL_3B_PATH)"
   ],
   "id": "f710b29ce4fe671b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8c67345907c4b47b98b7071e8700fd2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T21:04:59.570855Z",
     "start_time": "2024-12-06T21:04:58.182665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from tqdm import tqdm\n",
    "wandb.init(\n",
    "    project=\"seq2seq interior\",\n",
    "    config= {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"architecture\": \"transformer\",\n",
    "        \"dataset\": \"wikitext-2\",\n",
    "        \"epochs\": 1000,\n",
    "        \"loss\": \"cosine\"\n",
    "    }\n",
    ")"
   ],
   "id": "26b7c404ee1a8466",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mskimmer\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/sungwonkim/pycharm/MT Final/wandb/run-20241206_160459-xtor7wg9</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skimmer/seq2seq%20interior/runs/xtor7wg9' target=\"_blank\">fast-star-15</a></strong> to <a href='https://wandb.ai/skimmer/seq2seq%20interior' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skimmer/seq2seq%20interior' target=\"_blank\">https://wandb.ai/skimmer/seq2seq%20interior</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skimmer/seq2seq%20interior/runs/xtor7wg9' target=\"_blank\">https://wandb.ai/skimmer/seq2seq%20interior/runs/xtor7wg9</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/skimmer/seq2seq%20interior/runs/xtor7wg9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x16eaf6d20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T21:05:01.833070Z",
     "start_time": "2024-12-06T21:04:59.575414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "def load_data(max_length=512):\n",
    "    print(\"Loading dataset from HuggingFace...\")\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "    train_texts = dataset['train']['text']\n",
    "\n",
    "    filtered_texts = [text for text in train_texts if text.strip() and len(text.split()) <= max_length]\n",
    "    print(f\"Processed {len(filtered_texts)} samples after filtering\")\n",
    "\n",
    "    return filtered_texts\n",
    "texts = load_data()"
   ],
   "id": "d3b550fee6798918",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from HuggingFace...\n",
      "Processed 23758 samples after filtering\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T21:05:01.897936Z",
     "start_time": "2024-12-06T21:05:01.889462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from tqdm import tqdm\n",
    "wandb.init(\n",
    "    project=\"seq2seq interior\",\n",
    "    config= {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"architecture\": \"transformer\",\n",
    "        \"dataset\": \"wikitext-2\",\n",
    "        \"chunks\": 1000,\n",
    "        \"loss\": \"MSE\"\n",
    "    }\n",
    ")"
   ],
   "id": "ef139e587f5b9fa4",
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/skimmer/seq2seq%20interior/runs/xtor7wg9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x16eaf6d20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T21:16:26.149669Z",
     "start_time": "2024-12-06T21:05:18.327758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedder = TransformerEmbedder(3072, 2048, 2).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.CosineEmbeddingLoss()\n",
    "optimizer = torch.optim.Adam(embedder.parameters(), lr=0.001)\n",
    "embedder.train()\n",
    "losses = []\n",
    "\n",
    "for text in tqdm(texts[:1000]):\n",
    "    repr_1b, repr_3b = extractor.extract_intermediate_representations([text])\n",
    "    repr_1b = repr_1b[0].to(device)\n",
    "    repr_3b = repr_3b[0].to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = embedder(repr_3b)\n",
    "    loss = criterion(output.squeeze(), repr_1b.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    wandb.log({\"loss\": loss.item()})\n",
    "    losses.append(loss.item())"
   ],
   "id": "e8fbed56f4d5db9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [11:07<00:00,  1.50it/s]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T21:17:57.944978Z",
     "start_time": "2024-12-06T21:17:56.131609Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(embedder, \"transformer_embedder_1000.pth\")",
   "id": "4091d6e21dd01332",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:02:40.251921Z",
     "start_time": "2024-12-06T16:02:39.782282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedder.eval()\n",
    "repr_1b, repr_3b = extractor.extract_intermediate_representations([texts[999]])\n",
    "repr_1b = repr_1b[0].to(device)\n",
    "repr_3b = repr_3b[0].to(device)\n",
    "output = embedder(repr_3b)\n",
    "print(output.shape)"
   ],
   "id": "eba25d817406df85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22, 2048])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:02:40.258478Z",
     "start_time": "2024-12-06T16:02:40.255838Z"
    }
   },
   "cell_type": "code",
   "source": "print(repr_1b.shape)",
   "id": "c29c1aec760f9a31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22, 2048])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:02:40.312403Z",
     "start_time": "2024-12-06T16:02:40.300727Z"
    }
   },
   "cell_type": "code",
   "source": "print(output)",
   "id": "a10224ef8e0fe32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0502e+00, -2.4207e-01,  2.2764e+00,  ...,  6.0472e-01,\n",
      "           1.2637e+00,  8.9992e-01],\n",
      "         [-4.2429e-02, -1.3500e-02, -1.2751e-01,  ..., -4.1418e-02,\n",
      "          -5.6093e-02, -2.6518e-04],\n",
      "         [-4.2445e-02, -1.3496e-02, -1.2750e-01,  ..., -4.1411e-02,\n",
      "          -5.6087e-02, -2.5539e-04],\n",
      "         ...,\n",
      "         [-4.2491e-02, -1.3516e-02, -1.2748e-01,  ..., -4.1416e-02,\n",
      "          -5.6071e-02, -2.3512e-04],\n",
      "         [-4.2491e-02, -1.3516e-02, -1.2748e-01,  ..., -4.1416e-02,\n",
      "          -5.6071e-02, -2.3514e-04],\n",
      "         [-4.2491e-02, -1.3516e-02, -1.2748e-01,  ..., -4.1416e-02,\n",
      "          -5.6071e-02, -2.3513e-04]]], device='mps:0',\n",
      "       grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:02:40.359038Z",
     "start_time": "2024-12-06T16:02:40.347331Z"
    }
   },
   "cell_type": "code",
   "source": "print(repr_1b)",
   "id": "aa1af1d8bd209f2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1367, -0.1160,  0.9267,  ...,  0.4339,  0.7350,  0.5164],\n",
      "         [-0.1310, -0.1256, -0.2124,  ...,  0.1703, -0.1001,  0.0895],\n",
      "         [-0.2141, -0.0653, -0.1952,  ..., -0.0515, -0.0457,  0.0152],\n",
      "         ...,\n",
      "         [-0.1481, -0.0385, -0.1986,  ..., -0.0626, -0.0716, -0.0598],\n",
      "         [ 0.0657, -0.0803, -0.0886,  ..., -0.0153, -0.2196, -0.1318],\n",
      "         [ 0.0767, -0.1609, -0.0287,  ...,  0.0110, -0.0201,  0.0609]]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:02:40.428959Z",
     "start_time": "2024-12-06T16:02:40.393437Z"
    }
   },
   "cell_type": "code",
   "source": "print(criterion(output, repr_1b))",
   "id": "1bc243c9652018eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0655, device='mps:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:02:42.367896Z",
     "start_time": "2024-12-06T16:02:40.475282Z"
    }
   },
   "cell_type": "code",
   "source": "wandb.finish()",
   "id": "179380aaf81a261e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▂▁▂▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.0772</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dry-water-14</strong> at: <a href='https://wandb.ai/skimmer/seq2seq%20interior/runs/059f5vpp' target=\"_blank\">https://wandb.ai/skimmer/seq2seq%20interior/runs/059f5vpp</a><br/> View project at: <a href='https://wandb.ai/skimmer/seq2seq%20interior' target=\"_blank\">https://wandb.ai/skimmer/seq2seq%20interior</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20241206_104418-059f5vpp/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T16:02:42.451976Z",
     "start_time": "2024-12-06T16:02:42.450644Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9c70552fb3ea8bd8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
