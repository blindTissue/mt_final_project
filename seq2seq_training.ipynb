{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-06T10:15:02.446059Z",
     "start_time": "2024-12-06T10:15:01.856054Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int):\n",
    "        \"\"\"\n",
    "        Encoder for seq2seq model.\n",
    "        Args:\n",
    "            input_dim: Dimensionality of input vectors\n",
    "            hidden_dim: Hidden state size of the LSTM\n",
    "            num_layers: Number of LSTM layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_dim)\n",
    "        Returns:\n",
    "            encoder_outputs: Outputs for each time step (batch_size, seq_len, hidden_dim)\n",
    "            hidden: Tuple of (h_n, c_n) (last hidden and cell states)\n",
    "        \"\"\"\n",
    "        encoder_outputs, hidden = self.lstm(x)\n",
    "        return encoder_outputs, hidden\n",
    "\n",
    "\n",
    "class Seq2SeqDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, output_dim: int, num_layers: int):\n",
    "        \"\"\"\n",
    "        Decoder for seq2seq model.\n",
    "        Args:\n",
    "            hidden_dim: Dimensionality of the encoded hidden state\n",
    "            output_dim: Dimensionality of output vectors (same as input_dim)\n",
    "            num_layers: Number of LSTM layers (should match encoder)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor to the decoder (batch_size, seq_len, hidden_dim)\n",
    "            hidden: Tuple of (h_n, c_n) from the encoder\n",
    "        Returns:\n",
    "            outputs: Decoded sequence (batch_size, seq_len, output_dim)\n",
    "        \"\"\"\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        outputs = self.fc(lstm_out)\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, num_layers: int):\n",
    "        \"\"\"\n",
    "        Combines encoder and decoder into a seq2seq model.\n",
    "        Args:\n",
    "            input_dim: Dimensionality of input vectors\n",
    "            hidden_dim: Hidden state size\n",
    "            num_layers: Number of LSTM layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = Seq2SeqEncoder(input_dim, hidden_dim, num_layers)\n",
    "        self.decoder = Seq2SeqDecoder(hidden_dim, output_dim, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input sequence of shape (batch_size, seq_len, input_dim)\n",
    "        Returns:\n",
    "            output: Reconstructed sequence of shape (batch_size, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        encoder_outputs, hidden = self.encoder(x)\n",
    "\n",
    "        # Decoder (Use encoder outputs as initial input)\n",
    "        output, _ = self.decoder(encoder_outputs, hidden)\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:15:03.208539Z",
     "start_time": "2024-12-06T10:15:03.205834Z"
    }
   },
   "cell_type": "code",
   "source": "device = 'mps'",
   "id": "632ca238a828138d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:29:30.549203Z",
     "start_time": "2024-12-06T08:29:30.033419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "def load_data(max_length=512):\n",
    "    print(\"Loading dataset from HuggingFace...\")\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "    train_texts = dataset['train']['text']\n",
    "\n",
    "    filtered_texts = [text for text in train_texts if text.strip() and len(text.split()) <= max_length]\n",
    "    print(f\"Processed {len(filtered_texts)} samples after filtering\")\n",
    "\n",
    "    return filtered_texts"
   ],
   "id": "d7210206c031949d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:29:42.225713Z",
     "start_time": "2024-12-06T08:29:30.553103Z"
    }
   },
   "cell_type": "code",
   "source": "texts = load_data()",
   "id": "8070e0fad37a89cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from HuggingFace...\n",
      "Processed 23758 samples after filtering\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:15:07.551153Z",
     "start_time": "2024-12-06T10:15:07.272710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class LlamaIntermediateLayerExtractor:\n",
    "    def __init__(self, model_1b_path, model_3b_path):\n",
    "        \"\"\"\n",
    "        Initialize extractor for Llama 3.2 1b and 3b models\n",
    "\n",
    "        Args:\n",
    "            model_1b_path (str): Path or HuggingFace model ID for 1b model\n",
    "            model_3b_path (str): Path or HuggingFace model ID for 3b model\n",
    "        \"\"\"\n",
    "        # Load models and tokenizers\n",
    "        self.model_1b = AutoModelForCausalLM.from_pretrained(model_1b_path).to(device)\n",
    "        self.model_3b = AutoModelForCausalLM.from_pretrained(model_3b_path).to(device)\n",
    "\n",
    "        self.tokenizer_1b = AutoTokenizer.from_pretrained(\"tokenizers/meta-llama/llama-3.2-1B\")\n",
    "        self.tokenizer_3b = AutoTokenizer.from_pretrained(\"tokenizers/meta-llama/llama-3.2-3B\")\n",
    "\n",
    "        # Set models to evaluation mode\n",
    "        self.model_1b.eval()\n",
    "        self.model_3b.eval()\n",
    "\n",
    "        # Hooks to capture intermediate layer outputs\n",
    "        self.intermediate_output_1b = None\n",
    "        self.intermediate_output_3b = None\n",
    "\n",
    "    def _register_1b_hook(self):\n",
    "        \"\"\"Register hook for 1b model's 10th layer\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            self.intermediate_output_1b = output[0]  # Typically, first element is hidden states\n",
    "\n",
    "        # Assuming transformer layers are in model.model.layers or similar\n",
    "        # You might need to adjust this path based on your specific model structure\n",
    "        target_layer = self.model_1b.model.layers[9]  # 0-indexed, so 10th layer is index 9\n",
    "        self.hook_1b = target_layer.register_forward_hook(hook)\n",
    "\n",
    "    def _register_3b_hook(self):\n",
    "        \"\"\"Register hook for 3b model's 18th layer\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            self.intermediate_output_3b = output[0]\n",
    "\n",
    "        # Adjust this path based on your specific model structure\n",
    "        target_layer = self.model_3b.model.layers[17]  # 0-indexed, so 18th layer is index 17\n",
    "        self.hook_3b = target_layer.register_forward_hook(hook)\n",
    "\n",
    "    def extract_intermediate_representations(self, text_chunks, max_length=512):\n",
    "        \"\"\"\n",
    "        Extract intermediate representations for given text chunks\n",
    "\n",
    "        Args:\n",
    "            text_chunks (list): List of text chunks to process\n",
    "            max_length (int): Maximum token length to process\n",
    "\n",
    "        Returns:\n",
    "            tuple: (intermediate representations for 1b, intermediate representations for 3b)\n",
    "        \"\"\"\n",
    "        # Reset intermediate outputs\n",
    "        self.intermediate_output_1b = None\n",
    "        self.intermediate_output_3b = None\n",
    "\n",
    "        # Register hooks\n",
    "        self._register_1b_hook()\n",
    "        self._register_3b_hook()\n",
    "\n",
    "        # Prepare to collect representations\n",
    "        repr_1b_list = []\n",
    "        repr_3b_list = []\n",
    "\n",
    "        try:\n",
    "            for chunk in text_chunks:\n",
    "                # Tokenize and process 1b model\n",
    "                inputs_1b = self.tokenizer_1b(\n",
    "                    chunk,\n",
    "                    return_tensors='pt',\n",
    "                    truncation=True,\n",
    "                    max_length=max_length\n",
    "                ).to(device)\n",
    "\n",
    "                # Tokenize and process 3b model\n",
    "                inputs_3b = self.tokenizer_3b(\n",
    "                    chunk,\n",
    "                    return_tensors='pt',\n",
    "                    truncation=True,\n",
    "                    max_length=max_length\n",
    "                ).to(device)\n",
    "\n",
    "                # Forward pass to trigger hooks\n",
    "                with torch.no_grad():\n",
    "                    _ = self.model_1b(**inputs_1b)\n",
    "                    _ = self.model_3b(**inputs_3b)\n",
    "\n",
    "                # Store intermediate representations\n",
    "                if self.intermediate_output_1b is not None:\n",
    "                    repr_1b_list.append(self.intermediate_output_1b.detach())\n",
    "\n",
    "                if self.intermediate_output_3b is not None:\n",
    "                    repr_3b_list.append(self.intermediate_output_3b.detach())\n",
    "\n",
    "        finally:\n",
    "            # Remove hooks to prevent memory leaks\n",
    "            self.hook_1b.remove()\n",
    "            self.hook_3b.remove()\n",
    "\n",
    "        return repr_1b_list, repr_3b_list\n",
    "\n"
   ],
   "id": "f89959acbd22ca20",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:29:42.388252Z",
     "start_time": "2024-12-06T08:29:42.386921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MODEL_1B_PATH = \"models/meta-llama/llama-3.2-1B\"\n",
    "# MODEL_3B_PATH = \"models/meta-llama/llama-3.2-3B\"\n",
    "#\n",
    "# # Text chunks to process\n",
    "# text_chunks = [\n",
    "#     \"This is the first text chunk.\",\n",
    "#     \"Another interesting piece of text goes here.\",\n",
    "#     \"And a third chunk for good measure.\"\n",
    "# ]\n",
    "# extractor = LlamaIntermediateLayerExtractor(MODEL_1B_PATH, MODEL_3B_PATH)\n",
    "#\n",
    "# # Extract representations\n",
    "# repr_1b, repr_3b = extractor.extract_intermediate_representations(text_chunks)\n",
    "# print(len(repr_1b), len(repr_3b))\n",
    "# print(repr_1b[0].shape, repr_3b[0].shape)"
   ],
   "id": "b36d0d9520298d67",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:29:42.393008Z",
     "start_time": "2024-12-06T08:29:42.391752Z"
    }
   },
   "cell_type": "code",
   "source": "#print(repr_1b[1].shape, repr_3b[1].shape)",
   "id": "b3c48fbad6ca9678",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:29:42.397421Z",
     "start_time": "2024-12-06T08:29:42.396235Z"
    }
   },
   "cell_type": "code",
   "source": "from tqdm import tqdm",
   "id": "52a4f5a34187fa5b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:29:43.804293Z",
     "start_time": "2024-12-06T08:29:42.587524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "wandb.init(\n",
    "    project=\"seq2seq interior\",\n",
    "    config= {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"architecture\": \"LSTM\",\n",
    "        \"dataset\": \"wikitext-2\",\n",
    "        \"epochs\": 1000,\n",
    "        \"loss\": \"CosineEmbeddingLoss\"\n",
    "    }\n",
    ")"
   ],
   "id": "684dd22e8df6702d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mskimmer\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/sungwonkim/pycharm/MT Final/wandb/run-20241206_032943-grs30ih1</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skimmer/seq2seq%20interior/runs/grs30ih1' target=\"_blank\">cool-gorge-5</a></strong> to <a href='https://wandb.ai/skimmer/seq2seq%20interior' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/skimmer/seq2seq%20interior' target=\"_blank\">https://wandb.ai/skimmer/seq2seq%20interior</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/skimmer/seq2seq%20interior/runs/grs30ih1' target=\"_blank\">https://wandb.ai/skimmer/seq2seq%20interior/runs/grs30ih1</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/skimmer/seq2seq%20interior/runs/grs30ih1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1339eb590>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:15:36.969584Z",
     "start_time": "2024-12-06T10:15:18.816617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_1B_PATH = \"models/meta-llama/llama-3.2-1B\"\n",
    "MODEL_3B_PATH = \"models/meta-llama/llama-3.2-3B\"\n",
    "extractor = LlamaIntermediateLayerExtractor(MODEL_1B_PATH, MODEL_3B_PATH)"
   ],
   "id": "ac0d353c9ae56ce6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9268797611c4573b0bbde1e4a8760bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T10:16:17.939395Z",
     "start_time": "2024-12-06T10:16:16.367690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input = \"What is the meaning of life?\"\n",
    "repr_1b, repr_3b = extractor.extract_intermediate_representations([input])\n",
    "print(repr_3b[0])\n",
    "#sum\n",
    "print(repr_3b[0].sum())"
   ],
   "id": "3d32fc78f85486c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0601, -0.0200,  0.8885,  ...,  0.3583,  0.5194,  0.2792],\n",
      "         [ 0.2183, -0.0816, -0.4049,  ..., -0.2152,  0.1284,  0.4417],\n",
      "         [-0.1304, -0.2036, -0.3451,  ..., -0.2903,  0.1702, -0.1489],\n",
      "         ...,\n",
      "         [ 0.2089, -0.3997, -0.1467,  ..., -0.1077,  0.0199, -0.1851],\n",
      "         [-0.1151, -0.2551, -0.1698,  ..., -0.0078, -0.0934, -0.1084],\n",
      "         [ 0.0509, -0.1615,  0.1318,  ..., -0.1788, -0.2703, -0.4363]]],\n",
      "       device='mps:0')\n",
      "tensor(-628.8185, device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:46:09.959439Z",
     "start_time": "2024-12-06T08:31:01.281259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure hooks are removed and tensors are detached\n",
    "\n",
    "seq2seqmodel = Seq2SeqModel(3072, 2048, 2048, 1).to(device)\n",
    "#criterion = nn.MSELoss()\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "optimizer = torch.optim.Adam(seq2seqmodel.parameters(), lr=0.001)\n",
    "\n",
    "MODEL_1B_PATH = \"models/meta-llama/llama-3.2-1B\"\n",
    "MODEL_3B_PATH = \"models/meta-llama/llama-3.2-3B\"\n",
    "extractor = LlamaIntermediateLayerExtractor(MODEL_1B_PATH, MODEL_3B_PATH)\n",
    "average_5_loss = 0\n",
    "for i in tqdm(range(1000)):\n",
    "\n",
    "    text = [texts[i]]\n",
    "\n",
    "    # Extract representations with no_grad to save memory\n",
    "    with torch.no_grad():\n",
    "        repr_1b, repr_3b = extractor.extract_intermediate_representations(text)\n",
    "\n",
    "    seq2seqmodel.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Detach tensors to free up memory\n",
    "    repr_1b = [r.detach() for r in repr_1b]\n",
    "    repr_3b = [r.detach() for r in repr_3b]\n",
    "\n",
    "    out = seq2seqmodel(repr_3b[0])\n",
    "    loss = criterion(out.squeeze(), repr_1b[0].squeeze(), torch.ones(1).to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    average_5_loss += loss.item()\n",
    "    if i % 5 == 0:\n",
    "        wandb.log({\"loss\": average_5_loss / 5})\n",
    "        average_5_loss = 0"
   ],
   "id": "c314190cd475e94a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78f1593b5be54dfeaa2925e6f5b17b29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [14:52<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:11:01.500009Z",
     "start_time": "2024-12-06T08:11:01.497114Z"
    }
   },
   "cell_type": "code",
   "source": "seq2seqmodel",
   "id": "b96df88e561cd5c7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqModel(\n",
       "  (encoder): Seq2SeqEncoder(\n",
       "    (lstm): LSTM(3072, 2048, batch_first=True)\n",
       "  )\n",
       "  (decoder): Seq2SeqDecoder(\n",
       "    (lstm): LSTM(2048, 2048, batch_first=True)\n",
       "    (fc): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:51:07.299432Z",
     "start_time": "2024-12-06T08:51:06.628640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save the model\n",
    "torch.save(seq2seqmodel, \"seq2seqmodel_1000_cosine.pth\")"
   ],
   "id": "94412b26b4fccce6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:11:02.061392Z",
     "start_time": "2024-12-06T08:11:02.059258Z"
    }
   },
   "cell_type": "code",
   "source": "print(texts[722])",
   "id": "8dcecb0814e823ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the Guerrero district of Chihuahua , Pascual Orozco attacked Federal troops and sent dead soldiers ' clothing back to Díaz with the message , \" Ahí te van las hojas , mándame más tamales \" ( \" Here are the wrappers , send me more tamales . \" ) He then began operations which threatened Ciudad Juárez . Additionally , political support for Madero 's rebellion came from Abraham González , who accepted the Plan of San Luis Potosí . \n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:11:02.101733Z",
     "start_time": "2024-12-06T08:11:02.099860Z"
    }
   },
   "cell_type": "code",
   "source": "print(texts[967])\n",
   "id": "78ea3266cb904206",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = = Biography = = \n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:11:02.149911Z",
     "start_time": "2024-12-06T08:11:02.147869Z"
    }
   },
   "cell_type": "code",
   "source": "print(texts[914])\n",
   "id": "740da92936bbe3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Since no inscriptions on any of the island have been discovered , the ancient history of the island is conjectural , at best . Pandavas , the heroes of the Hindu epic Mahabharata , and Banasura , the demon devotee of Shiva , are both credited with building temples or cut caves to live . Local tradition holds that the caves are not man @-@ made . \n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:11:02.192154Z",
     "start_time": "2024-12-06T08:11:02.190240Z"
    }
   },
   "cell_type": "code",
   "source": "print(texts[915])\n",
   "id": "ab660e0c85c0c41c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Elephanta caves are \" of unknown date and attribution \" . Art historians have dated the caves in the range of late 5th to late 8th century AD . Archaeological excavations have unearthed a few Kshatrapa coins dated to 4th century AD . The known history is traced only to the defeat of Mauryan rulers of Konkan by the Badami Chalukyas emperor Pulakesi II ( 609 – 642 ) in a naval battle , in 635 AD . Elephanta was then called Puri or Purika , and served as the capital of the Konkan Mauryas . Some historians attribute the caves to the Konkan Mauryas , dating them to the mid @-@ 6th century , though others refute this claim saying a relatively small kingdom like the Konkan Mauryas could not undertake \" an almost superhuman excavation effort , \" which was needed to carve the rock temples from solid rock and could not have the skilled labor to produce such \" high quality \" sculpture . \n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:11:02.266159Z",
     "start_time": "2024-12-06T08:11:02.264188Z"
    }
   },
   "cell_type": "code",
   "source": "print(texts[916])\n",
   "id": "cdc94aed0e1da65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Some other historians attribute the construction to the Kalachuris ( late 5th to 6th century ) , who may have had a feudal relationship with the Konkan Mauryas . In an era where polytheism was prevalent , the Elephanta main cave dedicates the monotheism of the Pashupata Shaivism sect , a sect to which Kalachuris as well as Konkan Mauryas belonged . \n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:11:02.270731Z",
     "start_time": "2024-12-06T08:11:02.268556Z"
    }
   },
   "cell_type": "code",
   "source": "print(texts[917])\n",
   "id": "40bb8e982c181af6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Chalukyas , who defeated the Kalachuris as well as the Konkan Mauryas , are also believed by some to be creators of the main cave , in the mid @-@ 7th century . The Rashtrakutas are the last claimants to the creation of the main cave , approximated to the early 7th to late 8th century . The Elephanta Shiva cave resembles in some aspects the 8th @-@ century Rashtrakuta rock @-@ temple Kailash at Ellora . The Trimurti of Elephanta showing the three faces of Shiva is akin to the Trimurti of Brahma , Vishnu and Mahesh ( Shiva ) , which was the royal insignia of the Rashtrakutas . The Nataraja and Ardhanarishvara sculptures are also attributed to the Rashtrakutas . \n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T08:11:02.313578Z",
     "start_time": "2024-12-06T08:11:02.312319Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c87155a3bb976761",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
